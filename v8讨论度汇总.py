import requests
import bs4
import os
import re
import time
import urllib
import urllib.request
import random
from bs4 import BeautifulSoup
#æ´—æ´ç²¾ç±»
class Tool:
    removeImg = re.compile('<img.*?>| {7}|')
    removeAddr = re.compile('<a.*?>|</a>')
    replaceLine = re.compile('<tr>|<div>|</div>|</p>')
    replaceTD= re.compile('<td>')
    replacePara = re.compile('<p.*?>')
    replaceBR = re.compile('<br><br>|<br>')
    removeExtraTag = re.compile('<.*?>')
    #æ´—æ´ç²¾
    def replace(self,x):
        x = re.sub(self.removeImg,"",x)
        x = re.sub(self.removeAddr,"",x)
        x = re.sub(self.replaceLine,"\n",x)
        x = re.sub(self.replaceTD,"\t",x)
        x = re.sub(self.replacePara,"\n  ",x)
        x = re.sub(self.replaceBR,"\n",x)
        x = re.sub(self.removeExtraTag,"",x)
        return x.strip()
#å¸–å­è·å–ä¿¡æ¯ç±»
class BDTB(object):
    #seeLZ: 0:æŸ¥çœ‹æ‰€æœ‰å›å¤ 1:åªçœ‹æ¥¼ä¸»å›å¤
    def __init__(self, baseUrl, seeLZ):
        self.baseURL = baseUrl
        self.seeLZ = '?see_lz=' + str(seeLZ)
        self.tool = Tool()
    #è·å–å¸–å­é¡µé¢æ•°å†…å®¹
    def getPage(self, pageNum):
        try:
            url = self.baseURL + self.seeLZ + '&pn=' + str(pageNum)
            req = urllib.request.Request(url)
            response = urllib.request.urlopen(req)
            #print response.read()
            html = response.read().decode('utf-8')
            # print(html)
            return html
        except urllib.request.URLError as e:
            if hasattr(e, 'reason'):
                print ('è¿æ¥ç™¾åº¦è´´å§å¤±æ•ˆï¼Œé”™è¯¯åŸå› :', e.reason)
                return None
    #è·å–å¸–å­æ ‡é¢˜
    def getTitle(self, pageNum):
        page = self.getPage(pageNum)
        pat1 = re.compile(r'<h3 class="core_title_txt.*?>(.*?)</h3>', re.S)
        result = re.search(pat1, page)
        if result:
            #æ‰“å°title
            print("title:"+result.group(1))
            return(result.group(1))
        else:
            return None
    #è·å–å¸–å­å›å¤é¡µæ•°
    def getPageNum(self, pageNum):
        page = self.getPage(pageNum)
        pat1 = re.compile(r'<li class="l_reply_num".*?</span>.*?<span.*?>(.*?)</span>', re.S)
        result = re.search(pat1, page)
        if result:
            # print(result.group(1))
            return int(result.group(1))
        else:
            return None
    #è·å–å¸–å­å›å¤å†…å®¹
    def getContent(self, pageNum):
        page = self.getPage(pageNum)
        pat1 = re.compile(r'<div id="post_content.*?>(.*?)</div>', re.S)
        items = re.findall(pat1, page)
        stringc = []
        # for item in items:
        #     print (item)
        floor = 1
        for item in items:
            # print("")
            # print('ç¬¬%sé¡µçš„å¸–å­å†…å®¹:' %pageNum)
            # print(floor,u'æ¥¼')
            # æ‰“å°å›å¤è´´çš„å†…å®¹
            # print(self.tool.replace(item))
            stringc.append(self.tool.replace(item))
            floor += 1
        return stringc
#è´´å§ä¸»é¡µè·å–
#ç½‘å€çˆ¬è™«æ¡†æ¶
def get_html(url):
    try:
        # headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'}
        # kv={"usm":"3","rsv_idx":"2","rsv_page":"1"}
        r = requests.get(url, timeout=30)
        # proxies={'http':'36.248.132.13:9999'}
        # r = requests.get(url, timeout=30,proxies=proxies)
        r.raise_for_status()
        r.encoding = 'utf-8'
        # print(r.text)
        return r.text
    except:
        return " ERROR "
#å¾—åˆ°å¸–å­çš„å„é¡¹æ•°æ®
def get_content(url):
    comments = []
    html = get_html(url)
    soup = BeautifulSoup(html, 'lxml')
    liTags = soup.find_all('li', attrs={"class":['j_thread_list', 'clearfix']})
    for li in liTags:
        comment = {}
        try:
            comment['title'] = li.find(
                'a', attrs={"class": ['j_th_tit']}).text.strip()
            comment['link'] = "http://tieba.baidu.com/" + li.find('a', attrs={"class": ['j_th_tit']})['href']
            comment['name'] = li.find(
                'span', attrs={"class": ['tb_icon_author']}).text.strip()
            comment['time'] = li.find(
                'span', attrs={"class": ['pull-right is_show_create_time']}).text.strip()
            comment['replyNum'] = li.find(
                'span', attrs={"class": ['threadlist_rep_num center_text']}).text.strip()
            comments.append(comment)
        except:
            print('å‡ºäº†ç‚¹å°é—®é¢˜')
    return comments
#åˆ›é€ å·¥å…·å‡½æ•°
#åˆ›é€ è´´å§æ¯é¡µé“¾æ¥
def pagelist(num,base_url):
    url_list = []
    for p in range(0, num):
        i = random.randint(0, 201)
        # i=4
        url_list.append(base_url + '&pn=' + str(50 * i))
        print('No' + str(i) + "æ­£åœ¨ç­›é€‰")
        print(url_list)
    return url_list
#åˆ›é€ è´´å§æ¯é¡µå¸–å­çš„é“¾æ¥
def urllist(url_list):
    baseURL_list=[]
    for url in url_list:
        comments = get_content(url)
        for comment in comments:
            baseURL = comment['link']
            baseURL_list.append(baseURL)
    print(baseURL_list)
    return baseURL_list
#åˆ›é€ å¸–å­å›å¤ä¿¡æ¯
def tielist(url):
    strings = []
    baseURL=url
    bdtb = BDTB(baseURL,0)
    # bdtb.getTitle(1)
    page_num = bdtb.getPageNum(1)
    for i in range(page_num):
        # bdtb.getContent(i+1)
        strings += bdtb.getContent(i + 1)
        # print(strings)
    return strings
#ç»Ÿè®¡å·¥å…·ç±»
#è¿™æ®µè¯æåŠäº†è¿™äº›è™šæ‹Ÿä¸»æ’­å—
def vtb(str):
    mention=set()
    vtbs={
    #å›½ç±åˆ†ç±»
    "å›½v":["å›½v","ç†ŠçŒ«å¦¹"],
    "æ—¥v":["æ—¥v","yhm","æ—¥æœ¬v","æ¨±èŠ±å¦¹"],
    #ç®±åˆ†ç±»
    "asoul":["ä¹å","asoul","açƒ§","aéªš","Açƒ§","Aéªš","a-soul","A-soul","as","A-SOUL"],
    "p":["på®¶","å·´é»"],
    "holo":["holo","çŒ´æ¥¼","æ","æœ¨å£","å°¸ä½“","yagoo","YAGOO","cover","COVER","HOLO"],
    "nijisanji":["è™¹"],
    "vr":["vr","VR","å¾®é˜¿"],
    "monv": ["é­”å¥³"],
    #ç¼åˆäºº
    "meaqua":["meaqua","è¿ä½“","å…­å­—æ¯"],
    "ä¸ƒæµ·": ["å¨œå¨œç±³", "nanami", "æµ·å­å§", "ä¸ƒæµ·", "æµ·çš‡","å”¯ä¸€æŒ‡å®š","ybb"],

    #å•ä¸ªä¸»æ’­
    #asoul
    "diana": ["å˜‰", "ç„¶ç„¶", "æ´—è„šå©¢"],
    "xiangwan": ["å‘æ™š", "é¡¶ç¢—", "æ™šæ™š", "æ™šæŒ‡å¯¼"],
    "bella": ["è´æ‹‰", "åŒ—ææ˜Ÿ"],
    "carol": ["çˆ"],
    "nai0": ["ä¹ƒç³", "ä¹ƒ0"],
    #hololive
    "gawr":["é²¨"],
    "koko": ["tskk", "å¯å¯", "è™«çš‡", "è—", "é¾™çš‡", "æ†¨æ†¨é¾™", "ä¼šé•¿", "å°¸ä½“","coco","koko"],
    "lamy": ["èˆç±³", "ç»¿èŒ¶", "é›ªèŠ±","é›ªæ°‘","â„"],
    "aqua": ["å¤¸", "åœ£çš‡", "aqua", "å››å­—æ¯", "é˜¿åº“å¨…", "crew","ã‚ãã‚"],
    "haato": ["èµ¤äº•å¿ƒ", "å“ˆæ°ç›", "haato", "å¿ƒå¿ƒ"],
    "fubuki":["ç‹","fubuki","ç™½ä¸Š","å¦å…‹"],
    "pekora": ["å…”", "pekora", "ä½©å…‹æ‹‰","ğŸ‡","ğŸ°","ãºã“ã‚‰"],
    "heitao": ["é»‘æ¡ƒå½±", "æ¡ƒçš‡", "â™ ", "å¤§å¥ˆå¥ˆ", "å¤§è±è±","å˜»å˜»","è–¯æ¡","ğŸŸ"],
    "doris":["æœµå­å“¥","æœµè‰ä¸","salute"],
    "robo":["èåœ"],
    "rusia":["ç²½","éœ²è¥¿äºš"],
    "matsuri":["ç¥­","å¤è‰²"],
    "nene":["nene","æ¡ƒé“ƒ"],
    #på®¶
    "mea": ["mea", "ä¸‰å­—æ¯", "è´¢å¸ƒ", "ç¥æ¥½ã‚ã‚", "å¤©ç‹—", "å±‘å¥³äºº", "åŠäºº", "å¤©è‹Ÿ"],
    "serena": ["èŠ±å›­", "serena", "çŒ«çŒ«", "çŒ«çš‡"],
    "ritsu":["å¾‹"],
    "pariy": ["å¸•é‡Œ"],
    "oto": ["å¤§å§", "oto", "è€é˜¿å§¨", "ä¹™å¥³éŸ³"],
    "erio": ["å°çº¢", "è‰¾è‰æ¬§", "å®¦å®˜", "å¹»å®˜", "å¹»å£«", "èµ¤"],
    #nijisanji&VR
    "alice": ["å°å…”å­", "çˆ±ä¸½ä¸", "alice", "æœ‰æ –"],
    "mito": ["æœˆä¹‹ç¾å…”", "æœˆãƒ", "è¯æ°´å§"],
    "geye": ["è‘›"],
    "chitose": ["åƒå²"],
    "azi": ["æ¢“"],
    "aza": ["aza"],
    # å…¶ä»–
    "hareru":["èŠ±ä¸¸","å¤§å˜´"],
    "kelala":["å…‹æ‹‰æ‹‰"],
    "lili":["ç™½é“¶è‰è‰"],
    "tiantian":["å¤©å¤©"],
    "nyaru":["çŒ«é›·","å°çŒ«"],
    "zhanji":["æˆ˜å§¬","æ­Œå§¬"],
    "miemu":["å’©å§†"],
    "mieli":["å’©æ —"],
    "lulu":["lu","æ—¥è®°"],

    "nana":["ç‹—å¦ˆ","nana","è¾£è¾£","è¾›","ä¸ƒå¥ˆ","å¥ˆå¥ˆ","ç„¶å¦ˆ","ğŸ¶ğŸ´","ğŸ¶å¦ˆ","ğŸ¶ğŸ","æ–°å† ","å¨œå¨œ"],
    "haruka":["ç™½ç¥é¥","è±¹è±¹"],
    "nanoha":["èœç¾½"],
    "sutera":["èŠ±å®«"],
    "bt":["bt","å†°ç³–","BT"],
    "hiiro":["hiiro","ç‹ç‰›å¥¶"],
    "kitsuna":["è€çˆ±","çˆ±é…±","ç»Šçˆ±"],
    "yousa":["yousa","å†·é¸Ÿ"],
    "kyouha":["äº¬å"],
    "beren":["è´ä¼¦","å°è´","éå¸¸ã«"],
    "siiro":["å°ç™½"],
    "yui":["æ—¶é›¨"],
    "lanyin":["å…°éŸ³"],
    "nanako":["èœèœå­"],
    "sio":["æ±","æ˜Ÿå®«","æ˜Ÿå®®"],
    "niki":["miki","å¥ˆå§¬"],
    "lunai":["é¹¿"],
    "hanser":["æ†¨è‰²","hanser"],
    "shanoa":["å¤è¯ºé›…"],
    "sia":["å°èŒœ"],
    "chelsea":["åˆ‡èŒœå¨…"],
    "sara":["æ˜Ÿå·"],
    "sanri":["ä¸‰æ—¥","ä¸‰æ‹‰å¤«"],
    }
    for p in vtbs.values():
        for q in p:
            if q in str:
                mention.add(list(vtbs.keys())[list(vtbs.values()).index(p)])
                break
    return mention
def add(mention,value,vtbs):
    for p in mention:
        if p not in vtbs:
            vtbs[p]=0
            vtbs[p]+=value
        else:
            vtbs[p] += value
    return vtbs
if __name__ == '__main__':

    vtbs={'yousa':1, 'mieli':1, 'kyouha':1, 'meaqua':1, 'aza':4, 'lili':2, 'monv':2,
    'nai0':2, 'hanser':3, 'miemu':77, 'lanyin':5, 'sara':5, 'sia':5, 'hiiro':6, 'ritsu':7,
    'pariy':8,'niki':10, 'hanser:':10, 'carol':11, 'matsuri':13, 'doris':14, 'chitose':15,
    'sio':16, 'gawr':17, 'zhanji':20, 'siiro':21, 'geye':24, 'lunai':26, 'kitsuna':31,
    'lulu':34, 'robo':37, 'vr':38, 'kelala':44, 'haato':44, 'hareru':49, 'nyaru':50, 'lamy':50, 'bt':76,
    'p':82, 'heitao':85, 'oto':99, 'fubuki':127, 'serena':132, 'xiangwan':141, 'ä¸ƒæµ·':158, 'æ—¥v':166, 'å›½v':188,
    'pekora':188, 'alice':204, 'yui':204, 'erio':212, 'nijisanji':225, 'tiantian':250, 'nana':257, 'azi':306,
    'koko':380, 'beren':415, 'mea':590, 'asoul':604, 'bella':677, 'holo':816, 'aqua':821, 'diana':875}
    tiebaurl = 'http://tieba.baidu.com/f?kw=v&ie=utf-8'
    yemianurl=pagelist(1,tiebaurl)
    yemianurllist= urllist(yemianurl)
    for i in range(0,50):
        tieziurl=yemianurllist[i]
        # tielist(tieziurl)
        tieinfo=tielist(tieziurl)
        bdtb = BDTB(tieziurl, 0)
        theme=str(bdtb.getTitle(1) + tieinfo[0])
        if vtb(theme):
            vtbs=add(vtb(theme),len(tieinfo),vtbs)
            for tie in tieinfo:
                vtbs=add(vtb(tie),1,vtbs)
        else:
            for tie in tieinfo:
                vtbs=add(vtb(tie),1,vtbs)
        print(vtbs)
    #     # print("è¯·ç­‰å¾…10sec")
    #     # time.sleep(10.0)
    print(sorted(vtbs.items(), key=lambda d: d[1]))







